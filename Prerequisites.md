**Ollama CLI** for hosting your local models  

1. Install Ollama

```bash
curl -fsSL https://ollama.com/install.sh | sh

2. Start the Ollama server
```bash
ollama serve &
This runs the server in the background 

3. Pull your models
```bash
ollama pull llama3.2
ollama pull gemma7b
ollama pull mistral7b
ollama pull deepseek14b
ollama pull phi4
ollama pull qwen32b
ollama pull gemma27b
ollama pull deepseek32b
ollama pull llama3.3
