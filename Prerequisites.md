**Ollama CLI** for hosting your local models  

1. Install Ollama

curl -fsSL https://ollama.com/install.sh | sh

2. Start the Ollama server

ollama serve &
This runs the server in the background 

3. Pull your models

ollama pull llama3.2
ollama pull gemma7b
ollama pull mistral7b
ollama pull deepseek14b
ollama pull phi4
ollama pull qwen32b
ollama pull gemma27b
ollama pull deepseek32b
ollama pull llama3.3
